{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81c21f3d-cfef-46de-859f-f3598e51b210",
   "metadata": {},
   "source": [
    "# 1. Binary Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5484e6f-f0ee-4c47-ad44-03555ddd0990",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn \n",
    "from sklearn.linear_model import SGDClassifier "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609a296c-7d25-4df5-8dc7-4a7145b8de49",
   "metadata": {},
   "source": [
    "### 1a. SGD Classifier\n",
    "Apply the SGD Classifier - this is capable of handling very large data sets efficiently becuase SGD deals with training instances independently one at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc499957-001d-4f1f-a8d4-4a3b555b7680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]\n",
    "\n",
    "# Turn into a binary classification problem - either y=5 or y!=5\n",
    "y_train_5 = (y_train == 5)\n",
    "y_test_5 = (y_test == 5)\n",
    "\n",
    "# Run SGD (Stochastic Gradient Descent) Classifier \n",
    "sgd_clf = SGDClassifier(max_iter=1000, tol=1e-3, random_state=42)\n",
    "sgd_clf.fit(X_train, y_train_5)\n",
    "\n",
    "# Prediction\n",
    "sgd_clf.predict([some_digit])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df99ea83-5c40-4b06-a5ba-d644d93520b6",
   "metadata": {},
   "source": [
    "### 1b. Performance Measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4672adf9-f08d-4c62-b2a5-7661b1483351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Stratified Cross-Validation and print the cross validaiton score\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.base import clone\n",
    "\n",
    "skfolds = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "for train_index, test_index in skfolds.split(X_train, y_train_5):\n",
    "    clone_clf = clone(sgd_clf)\n",
    "    X_train_folds = X_train[train_index]\n",
    "    y_train_folds = y_train_5[train_index]\n",
    "    X_test_fold = X_train[test_index]\n",
    "    y_test_fold = y_train_5[test_index]\n",
    "\n",
    "    clone_clf.fit(X_train_folds, y_train_folds)\n",
    "    y_pred = clone_clf.predict(X_test_fold)\n",
    "    n_correct = sum(y_pred == y_test_fold)\n",
    "    print(n_correct / len(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e2ba6e-7edf-41f0-b581-b0997769e7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use sklearn to obtain cross validation score \n",
    "# This will output an accuracy score for each fold (in this case 3)\n",
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(sgd_clf, X_train, y_train_5, cv=3, scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ef64c5-fd84-42f2-8654-749b5f598478",
   "metadata": {},
   "source": [
    "### 1c. Compare Against a Base Estimator\n",
    "This will compare against a base estimator (dumb classifier) that classifies every single image to be not \"5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bf979b-c528-414f-bede-19598704d118",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "# Create the dumb classifier\n",
    "class Never5Classifier(BaseEstimator):\n",
    "    def fit(self, X, y=None):\n",
    "        pass\n",
    "    def predict(self, X):\n",
    "        return np.zeros((len(X), 1), dtype=bool)\n",
    "    \n",
    "# Test results of classifier\n",
    "never_5_clf = Never5Classifier()\n",
    "cross_val_score(never_5_clf, X_train, y_train_5, cv=3, scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1111b3-ddea-4893-bbae-f0c7f9f1aa88",
   "metadata": {},
   "source": [
    "Note: accuracy is not the preferred performance measure for classifiers when dealing with skewed datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7fa78b-ec50-43d3-830b-d38f27a7dc50",
   "metadata": {},
   "source": [
    "### 1d. Confusion Matrix\n",
    "Confusion Matrix is a much better way to evaluate classifier performance. The `cross_val_predict()` function will perform K-fold cross-validation, but returns the predictions made on each test fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d20b8c3-5f6e-4768-8409-3848ba4fc153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First lets create a set of predictions that we can use to compare against the actual training data\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "y_train_pred = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3)\n",
    "\n",
    "# Create Confusion Matrix by comparing actual to predicted data\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_train_5, y_train_pred) #y_train_5 is a vector of actual y values "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38dc0ae1-eaf6-4fce-89d8-e280ebdccdf2",
   "metadata": {},
   "source": [
    "* Each Row = actual class\n",
    "* Each Column = predicted class\n",
    "* Top Left = true negatives ; Top Right = false positives\n",
    "* Bottom Left = false negatives ; Bottom Right = true positives\n",
    "* Precision = TP / TP+FP -- accuracy of positive predictions\n",
    "* Recall or Sensitivity or True Positive Rate = TP / TP + FN -- ratio of positive instances that are correctly detected by classifier  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69f0ad3-3f48-49db-81a4-7d4ad638c609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Precision and Recall with sklearn \n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "precision_score(y_train_5, y_train_pred)\n",
    "recall_score(y_train_5, y_train_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eca9097-6a2b-4cd4-b792-e8383a9708f3",
   "metadata": {},
   "source": [
    "* F1 Score combines precision and recall into a single metric if you need to compare two classifiers. F1 score is the harmonic mean of precision and recall. Harmonic mean gives much more weight to low values. Classifiers will therefore only get a high F1 score if both recall and precision are hgh \n",
    "* Formula: F1 = 2 $\\cdot$ $\\frac{precision \\times recall}{precision + recall}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed2ad74-8c25-4f2a-b9f9-8215f925e64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate F1 Score\n",
    "from sklearn.metrics import f1_score\n",
    "f1_score(y_train_5, y_train_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0ad9c1-5ed9-40c6-9006-823e53a73c59",
   "metadata": {},
   "source": [
    "### 1e. Precision/Recall Tradeoff\n",
    "The code below can be used to visualize this tradeoff and select a threshold to use in your classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d5d328-de50-418a-99d1-27a9cb83b187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the decision scores for every data point\n",
    "y_scores = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3,\n",
    "                             method=\"decision_function\")\n",
    "\n",
    "# Generate a curve showing the precision and recall for all possible thresholds\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_train_5, y_scores)\n",
    "\n",
    "def plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n",
    "    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\", linewidth=2)\n",
    "    plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\", linewidth=2)\n",
    "    plt.legend(loc=\"center right\", fontsize=16) # Not shown in the book\n",
    "    plt.xlabel(\"Threshold\", fontsize=16)        # Not shown\n",
    "    plt.grid(True)                              # Not shown\n",
    "    plt.axis([-50000, 50000, 0, 1])             # Not shown\n",
    "    \n",
    "recall_90_precision = recalls[np.argmax(precisions >= 0.90)]\n",
    "threshold_90_precision = thresholds[np.argmax(precisions >= 0.90)]\n",
    "\n",
    "plt.figure(figsize=(8, 4))                                                                  # Not shown\n",
    "plot_precision_recall_vs_threshold(precisions, recalls, thresholds)\n",
    "plt.plot([threshold_90_precision, threshold_90_precision], [0., 0.9], \"r:\")                 # Not shown\n",
    "plt.plot([-50000, threshold_90_precision], [0.9, 0.9], \"r:\")                                # Not shown\n",
    "plt.plot([-50000, threshold_90_precision], [recall_90_precision, recall_90_precision], \"r:\")# Not shown\n",
    "plt.plot([threshold_90_precision], [0.9], \"ro\")                                             # Not shown\n",
    "plt.plot([threshold_90_precision], [recall_90_precision], \"ro\")                             # Not shown\n",
    "save_fig(\"precision_recall_vs_threshold_plot\")                                              # Not shown\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a17202b-7e78-4b47-935f-efce91c7e800",
   "metadata": {},
   "source": [
    "Another way to select a good precision/recall trade-off is to plot precision directly against recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c766cb85-fed5-479a-8520-692ce725688e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot precision against recall\n",
    "def plot_precision_vs_recall(precisions, recalls):\n",
    "    plt.plot(recalls, precisions, \"b-\", linewidth=2)\n",
    "    plt.xlabel(\"Recall\", fontsize=16)\n",
    "    plt.ylabel(\"Precision\", fontsize=16)\n",
    "    plt.axis([0, 1, 0, 1])\n",
    "    plt.grid(True)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plot_precision_vs_recall(precisions, recalls)\n",
    "plt.plot([recall_90_precision, recall_90_precision], [0., 0.9], \"r:\") # Not need\n",
    "plt.plot([0.0, recall_90_precision], [0.9, 0.9], \"r:\") # Not needed\n",
    "plt.plot([recall_90_precision], [0.9], \"ro\")\n",
    "save_fig(\"precision_vs_recall_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c059e49-3a38-4efd-9bfd-a9c1d3e7e0fd",
   "metadata": {},
   "source": [
    "### 1f. ROC Curve\n",
    "This plots the TPR (Sensitivty or Recall) against the FPR, where FPR is the ratio of negative instances that are incorrectly classified as positive; in other terms the FPR = 1 - TNR = 1 - specificity\n",
    "* **Note:** you should use a precision-recall curve if your data has very few instances of positive values or when you care more about false-positives than false negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0190bd-7618-44e7-a513-8659a600e529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC Curve\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_train_5, y_scores)\n",
    "\n",
    "def plot_roc_curve(fpr, tpr, label=None):\n",
    "    plt.plot(fpr, tpr, linewidth=2, label=label)\n",
    "    plt.plot([0, 1], [0, 1], 'k--') # dashed diagonal\n",
    "    plt.axis([0, 1, 0, 1])                                    # Not shown in the book\n",
    "    plt.xlabel('False Positive Rate (Fall-Out)', fontsize=16) # Not shown\n",
    "    plt.ylabel('True Positive Rate (Recall)', fontsize=16)    # Not shown\n",
    "    plt.grid(True)                                            # Not shown\n",
    "\n",
    "plt.figure(figsize=(8, 6))                                    # Not shown\n",
    "plot_roc_curve(fpr, tpr)\n",
    "fpr_90 = fpr[np.argmax(tpr >= recall_90_precision)]           # Not shown\n",
    "plt.plot([fpr_90, fpr_90], [0., recall_90_precision], \"r:\")   # Not needed\n",
    "plt.plot([0.0, fpr_90], [recall_90_precision, recall_90_precision], \"r:\")  # Not needed\n",
    "plt.plot([fpr_90], [recall_90_precision], \"ro\")               # Not shown\n",
    "save_fig(\"roc_curve_plot\")                                    # Not shown\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5887b485-d1ed-4ede-8af5-0047024f7962",
   "metadata": {},
   "source": [
    "### 1g. Aread Under the Curve (AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e37f0182-91b3-405c-bf57-9ba1ac22b1f6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_train_5' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m roc_auc_score\n\u001b[0;32m----> 2\u001b[0m roc_auc_score(\u001b[43my_train_5\u001b[49m, y_scores)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_train_5' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(y_train_5, y_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57fadad-99b3-49b8-b139-7ecc2b057c17",
   "metadata": {},
   "source": [
    "# 2. Multiclass Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e74d44-cce4-47fa-85ae-b93a4a577b68",
   "metadata": {},
   "source": [
    "* Logistic Regression, RF, Naive Bayes classifiers can handle multiclass natively \n",
    "* SGD, SVM classifiers are strictly binary classifiers, but you can perform multiclass classification with multiple binary classifiers\n",
    "* One-vs-Rest (OvR) strategy is typically preferred over One-vs-One (OvO) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70dd6059-3df2-4ad9-9971-a84b7488a957",
   "metadata": {},
   "source": [
    "### 2a. SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afd7081-0a4f-47e9-ad39-3a8c4e072477",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# SVM example for multi-class - actually does OvO under the hood\n",
    "# 45 binary classifiers (OvO is usually done just for SVM)\n",
    "svm_clf = SVC(gamma=\"auto\", random_state=42)\n",
    "svm_clf.fit(X_train[:1000], y_train[:1000]) # y_train, not y_train_5\n",
    "svm_clf.predict([some_digit])\n",
    "\n",
    "# Get the decision function scores for each digit\n",
    "some_digit_scores = svm_clf.decision_function([some_digit])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b2c2c7-a09d-4244-8511-e2dcb79257f9",
   "metadata": {},
   "source": [
    "Force sklearn to use OvR rather than OvO method for SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c912c859-8d0e-4c78-b691-18a0a64cae7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force SVM to use OvR method\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "ovr_clf = OneVsRestClassifier(SVC(gamma=\"auto\", random_state=42))\n",
    "ovr_clf.fit(X_train[:1000], y_train[:1000])\n",
    "ovr_clf.predict([some_digit])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7918192-393c-4db4-9c87-79d9b8053d0c",
   "metadata": {},
   "source": [
    "### 2b. SGD Multiclass (OvR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2def1bae-0154-406a-aca3-be218d2c32e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same logic as binary classification problem \n",
    "# Does OvR under the hood \n",
    "sgd_clf.fit(X_train, y_train)\n",
    "sgd_clf.predict([some_digit])\n",
    "\n",
    "# Return the decision function for each class\n",
    "sgd_clf.decision_function([some_digit])\n",
    "\n",
    "# Now let's use cross validation\n",
    "cross_val_score(sgd_clf, X_train, y_train, cv=3, scoring=\"accuracy\")\n",
    "\n",
    "# Scaling the inputs will increase the accuracy\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train.astype(np.float64))\n",
    "cross_val_score(sgd_clf, X_train_scaled, y_train, cv=3, scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa114a1e-25fe-426e-accf-2c98beb135f5",
   "metadata": {},
   "source": [
    "### 2c. Error Analysis\n",
    "* This is done after you have selected a model and want to analyze the types of errors that it makes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1063c3-5a1d-4a85-93e2-07d76a327962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First lets look at the confusion matrix\n",
    "y_train_pred = cross_val_predict(sgd_clf, X_train_scaled, y_train, cv=3)\n",
    "conf_mx = confusion_matrix(y_train, y_train_pred)\n",
    "\n",
    "# Plot this confusion matrix - ABSOLUTE ERRORs\n",
    "plt.matshow(conf_mx, cmap=plt.cm.gray)\n",
    "save_fig(\"confusion_matrix_plot\", tight_layout=False)\n",
    "plt.show()\n",
    "\n",
    "# Plot the confusion matrix - ERROR RATE - what you want\n",
    "row_sums = conf_mx.sum(axis=1, keepdims=True)\n",
    "norm_conf_mx = conf_mx / row_sums\n",
    "np.fill_diagonal(norm_conf_mx, 0)\n",
    "plt.matshow(norm_conf_mx, cmap=plt.cm.gray)\n",
    "save_fig(\"confusion_matrix_errors_plot\", tight_layout=False)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
